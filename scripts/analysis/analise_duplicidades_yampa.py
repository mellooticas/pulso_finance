#!/usr/bin/env python3
"""
AnÃ¡lise de Duplicidades - Yampa RelatÃ³rios
==========================================

Este script analisa os arquivos yampa_relatorio.csv e yampa_relatorio_antigo.csv
para identificar duplicidades e preparar dados limpos para inserÃ§Ã£o no banco.

CritÃ©rios de anÃ¡lise:
1. Duplicatas exatas (todas as colunas iguais)
2. Duplicatas por chaves Ãºnicas (data + histÃ³rico + valor)
3. SobreposiÃ§Ã£o temporal entre arquivos
4. InconsistÃªncias nos dados
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import hashlib

# ConfiguraÃ§Ãµes
DATA_DIR = Path("../../data/base")
ARQUIVO_ATUAL = DATA_DIR / "yampa_relatorio.csv"
ARQUIVO_ANTIGO = DATA_DIR / "yampa_relatorio_antigo.csv"

def load_csv_safe(filepath, encoding='utf-8'):
    """Carrega CSV com tratamento de erros de encoding"""
    try:
        return pd.read_csv(filepath, sep=';', encoding=encoding)
    except UnicodeDecodeError:
        print(f"Tentando encoding latin1 para {filepath}")
        return pd.read_csv(filepath, sep=';', encoding='latin1')
    except Exception as e:
        print(f"Erro ao carregar {filepath}: {e}")
        return None

def clean_dataframe(df):
    """Limpa e padroniza o DataFrame"""
    if df is None:
        return None
    
    # Remove espaÃ§os extras das colunas string
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].astype(str).str.strip()
    
    # Converte datas
    date_columns = ['Data esperada', 'Data realizada', 'Data da Ãºltima alteraÃ§Ã£o']
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], format='%d/%m/%Y', errors='coerce')
    
    # Converte valores monetÃ¡rios
    value_columns = ['Valor a receber', 'Valor recebido', 'Valor a pagar', 'Valor pago']
    for col in value_columns:
        if col in df.columns:
            # Remove vÃ­rgulas e converte para float
            df[col] = df[col].astype(str).str.replace(',', '.').replace('', '0')
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    
    return df

def create_unique_key(row):
    """Cria uma chave Ãºnica baseada nos campos principais"""
    key_fields = [
        str(row.get('Data realizada', '')),
        str(row.get('HistÃ³rico', '')),
        str(row.get('Valor a pagar', 0)),
        str(row.get('Valor pago', 0)),
        str(row.get('Valor a receber', 0)),
        str(row.get('Valor recebido', 0)),
        str(row.get('Plano de contas', ''))
    ]
    key_string = '|'.join(key_fields)
    return hashlib.md5(key_string.encode()).hexdigest()

def analyze_duplicates():
    """FunÃ§Ã£o principal de anÃ¡lise"""
    print("ğŸ” ANÃLISE DE DUPLICIDADES - YAMPA RELATÃ“RIOS")
    print("=" * 60)
    
    # 1. Carregar arquivos
    print("\nğŸ“‚ Carregando arquivos...")
    df_atual = load_csv_safe(ARQUIVO_ATUAL)
    df_antigo = load_csv_safe(ARQUIVO_ANTIGO)
    
    if df_atual is None or df_antigo is None:
        print("âŒ Erro ao carregar arquivos!")
        return
    
    print(f"âœ… Arquivo atual: {len(df_atual):,} registros")
    print(f"âœ… Arquivo antigo: {len(df_antigo):,} registros")
    
    # 2. Limpar dados
    print("\nğŸ§¹ Limpando dados...")
    df_atual_clean = clean_dataframe(df_atual.copy())
    df_antigo_clean = clean_dataframe(df_antigo.copy())
    
    # 3. Adicionar chaves Ãºnicas
    print("\nğŸ”‘ Criando chaves Ãºnicas...")
    df_atual_clean['unique_key'] = df_atual_clean.apply(create_unique_key, axis=1)
    df_antigo_clean['unique_key'] = df_antigo_clean.apply(create_unique_key, axis=1)
    
    # 4. AnÃ¡lise de duplicatas
    print("\nğŸ“Š ANÃLISE DE DUPLICATAS")
    print("-" * 40)
    
    # Duplicatas dentro do arquivo atual
    duplicatas_atual = df_atual_clean[df_atual_clean.duplicated(subset=['unique_key'], keep=False)]
    print(f"ğŸ“‹ Duplicatas no arquivo atual: {len(duplicatas_atual):,}")
    
    # Duplicatas dentro do arquivo antigo
    duplicatas_antigo = df_antigo_clean[df_antigo_clean.duplicated(subset=['unique_key'], keep=False)]
    print(f"ğŸ“‹ Duplicatas no arquivo antigo: {len(duplicatas_antigo):,}")
    
    # Duplicatas entre arquivos
    keys_atual = set(df_atual_clean['unique_key'])
    keys_antigo = set(df_antigo_clean['unique_key'])
    keys_comum = keys_atual.intersection(keys_antigo)
    
    print(f"ğŸ”„ Registros em comum entre arquivos: {len(keys_comum):,}")
    print(f"ğŸ“ˆ Registros Ãºnicos no atual: {len(keys_atual - keys_antigo):,}")
    print(f"ğŸ“‰ Registros Ãºnicos no antigo: {len(keys_antigo - keys_atual):,}")
    
    # 5. AnÃ¡lise temporal
    print("\nğŸ“… ANÃLISE TEMPORAL")
    print("-" * 40)
    
    if not df_atual_clean['Data realizada'].isna().all():
        data_min_atual = df_atual_clean['Data realizada'].min()
        data_max_atual = df_atual_clean['Data realizada'].max()
        print(f"ğŸ“Š Arquivo atual: {data_min_atual.date()} atÃ© {data_max_atual.date()}")
    
    if not df_antigo_clean['Data realizada'].isna().all():
        data_min_antigo = df_antigo_clean['Data realizada'].min()
        data_max_antigo = df_antigo_clean['Data realizada'].max()
        print(f"ğŸ“Š Arquivo antigo: {data_min_antigo.date()} atÃ© {data_max_antigo.date()}")
    
    # 6. Criar dataset consolidado
    print("\nğŸ”„ CRIANDO DATASET CONSOLIDADO")
    print("-" * 40)
    
    # Combinar DataFrames
    df_atual_clean['fonte'] = 'atual'
    df_antigo_clean['fonte'] = 'antigo'
    
    df_combined = pd.concat([df_atual_clean, df_antigo_clean], ignore_index=True)
    
    # Remover duplicatas (manter o mais recente - do arquivo atual)
    df_consolidated = df_combined.sort_values(['fonte', 'Data da Ãºltima alteraÃ§Ã£o'], ascending=[True, False])
    df_consolidated = df_consolidated.drop_duplicates(subset=['unique_key'], keep='first')
    
    print(f"ğŸ“Š Total antes da consolidaÃ§Ã£o: {len(df_combined):,}")
    print(f"âœ… Total apÃ³s remoÃ§Ã£o de duplicatas: {len(df_consolidated):,}")
    print(f"ğŸ—‘ï¸ Duplicatas removidas: {len(df_combined) - len(df_consolidated):,}")
    
    # 7. AnÃ¡lise de qualidade
    print("\nğŸ¯ ANÃLISE DE QUALIDADE DOS DADOS")
    print("-" * 40)
    
    # Campos obrigatÃ³rios faltantes
    campos_obrigatorios = ['Data realizada', 'HistÃ³rico', 'Plano de contas']
    for campo in campos_obrigatorios:
        nulos = df_consolidated[campo].isna().sum()
        print(f"âŒ {campo}: {nulos:,} registros nulos ({nulos/len(df_consolidated)*100:.1f}%)")
    
    # Valores zerados
    valores_zero = (
        (df_consolidated['Valor a receber'] == 0) & 
        (df_consolidated['Valor recebido'] == 0) & 
        (df_consolidated['Valor a pagar'] == 0) & 
        (df_consolidated['Valor pago'] == 0)
    ).sum()
    print(f"âš ï¸ Registros com todos os valores zerados: {valores_zero:,}")
    
    # 8. Salvar resultado
    output_file = DATA_DIR / "yampa_consolidated.csv"
    df_final = df_consolidated.drop(['unique_key', 'fonte'], axis=1)
    df_final.to_csv(output_file, sep=';', index=False, encoding='utf-8')
    
    print(f"\nğŸ’¾ Arquivo consolidado salvo em: {output_file}")
    print(f"ğŸ“Š Registros finais: {len(df_final):,}")
    
    # 9. Resumo para SQL
    print("\nğŸ“‹ RESUMO PARA GERAÃ‡ÃƒO SQL")
    print("-" * 40)
    print(f"âœ… Arquivo limpo: yampa_consolidated.csv")
    print(f"ğŸ“Š Total de registros: {len(df_final):,}")
    print(f"ğŸ“… PerÃ­odo: {df_final['Data realizada'].min().date()} a {df_final['Data realizada'].max().date()}")
    
    # EstatÃ­sticas por tipo
    receitas = len(df_final[(df_final['Valor a receber'] > 0) | (df_final['Valor recebido'] > 0)])
    despesas = len(df_final[(df_final['Valor a pagar'] > 0) | (df_final['Valor pago'] > 0)])
    
    print(f"ğŸ’° Receitas: {receitas:,} registros")
    print(f"ğŸ’¸ Despesas: {despesas:,} registros")
    
    return df_final

if __name__ == "__main__":
    resultado = analyze_duplicates()